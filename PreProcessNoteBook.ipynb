{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('SinhalaSpellChecker': conda)"
  },
  "interpreter": {
   "hash": "dfd7741fd10dbdafac6de92b706d283abdbabb2e94c221243fc4b09e345fee76"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "def file_extractor(folder_path_lst, ofile):\r\n",
    "    f_out =  open(ofile, mode= 'w', encoding='utf8')\r\n",
    "    for folder_path in folder_path_lst:\r\n",
    "        for path, subdirs, files in os.walk(folder_path):\r\n",
    "            for f in files: \r\n",
    "                if '.txt' == f[-4:]:\r\n",
    "                    with open(os.path.join(path, f), mode= 'r', encoding='utf8') as f:\r\n",
    "                        f_out.writelines(f.readlines())\r\n",
    "    f_out.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from preProcessing import *\r\n",
    "from preProcessing import correct_words\r\n",
    "from scRNN.error_word_generation import AddNoise\r\n",
    "import json\r\n",
    "\r\n",
    "ofile_path = 'dataset/train/14_JUN/text0.txt'\r\n",
    "file_path = 'dataset/train/14_JUN/නිර්ව්‍යාජ බුද්ධිමතෙකුගේ.txt'\r\n",
    "clean_path = 'dataset/PreProcessing/cleaning.json'\r\n",
    "\r\n",
    "with open(file_path, mode='r', encoding='utf8') as f:\r\n",
    "    text = f.read()\r\n",
    "with open(clean0_path, mode='r', encoding='utf8') as f:\r\n",
    "    rules_dict = json.load(f)\r\n",
    "    rules_dict = {k : v for k,v in rules_dict.items() if v != None}\r\n",
    "\r\n",
    "text = unicode_error_correction(text, False)\r\n",
    "text = correct_words(text, rules_dict)\r\n",
    "\r\n",
    "with open(ofile_path, mode='w+', encoding='utf8') as f:\r\n",
    "    f.write(text)\r\n",
    "import json\r\n",
    "AddNoise_obj = AddNoise.getAddNoiseObj(ofile_path)\r\n",
    "AddNoise_obj.addNoiseFile('dataset/train/14_JUN')\r\n",
    "\r\n",
    "with open(ofile_path.split('.')[-1]+ '.json', mode= 'w+', encoding='utf8') as f:\r\n",
    "     json.dump({k :v for k, v in sorted(AddNoise_obj.freq().items(), key=lambda item: item[0])}, f, indent= 4, ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "from scRNN.error_word_generation import AddNoise\r\n",
    "file_path  = 'dataset/train/10_JUN/ucleaned_data.txt'\r\n",
    "AddNoise_obj = AddNoise.getAddNoiseObj(file_path)\r\n",
    "file_path  = 'dataset/Other/data_ctest_good.txt'\r\n",
    "AddNoise_obj_1 = AddNoise.getAddNoiseObj(file_path)\r\n",
    "file_path  = 'dataset/Other/data_ctest_good_copy.txt'\r\n",
    "AddNoise_obj_2 = AddNoise.getAddNoiseObj(file_path)\r\n",
    "set_a = {k  for k, v in AddNoise_obj.freq().items()}\r\n",
    "set_b = {k  for k, v in AddNoise_obj_1.freq().items()}\r\n",
    "set_c = {k  for k, v in AddNoise_obj_2.freq().items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(list(set_b - (set_b - (set_b).intersection(set_c)))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set_b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set_b - (set_b - set_c))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set_b - set_b.intersection((set_c).union(set_a)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sorted(list(set_b - set_b.intersection((set_c).union(set_a))))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\r\n",
    "This code is to count the number of lines in the copra\r\n",
    "\"\"\"\r\n",
    "import os\r\n",
    "from preProcessing import get_words\r\n",
    "folder = \"dataset/train/text_books/text_book_copra\"\r\n",
    "line_count = 0\r\n",
    "for file in os.listdir(folder):\r\n",
    "    with open(os.path.join(folder, file),mode='r', encoding='utf8') as f:\r\n",
    "        line_count = line_count + len(f.readlines())\r\n",
    "print(\"Number of sentences : \", line_count)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute word similiarity with the train set test set and indvidual test sets\r\n",
    "import os\r\n",
    "import csv\r\n",
    "import json\r\n",
    "from preProcessing import get_words\r\n",
    "\r\n",
    "isolated_test_folder = 'dataset/test_00_good'\r\n",
    "isolated_test_cases = {}\r\n",
    "for file in os.listdir(isolated_test_folder):\r\n",
    "    isolated_test_cases[file] = get_words(os.path.join(isolated_test_folder,  file), '[^\\u0D80-\\u0DFF\\u200a-\\u200d]')\r\n",
    "\r\n",
    "with open(file= 'C:/Users/chara/Documents/Work/RE_CS/neuspell/data/checkpoints/scrnn-none/vocab.json' , mode='r' , encoding='utf8') as f:\r\n",
    "    vocab = [t for t in json.load(f)['token2idx'].keys()]\r\n",
    "\r\n",
    "\r\n",
    "test_file = \"dataset/Other/data_test_good.txt\"\r\n",
    "test_corpus = get_words(test_file, '[^\\u0D80-\\u0DFF\\u200a-\\u200d]')\r\n",
    "\r\n",
    "train_file = \"dataset/train/10_JUN/ucleaned_data.txt\"\r\n",
    "train_corpus_0 = get_words(train_file, '[^\\u0D80-\\u0DFF\\u200a-\\u200d]')\r\n",
    "train_file = \"dataset/Other/train8.txt\"\r\n",
    "train_corpus_1 = get_words(train_file, '[^\\u0D80-\\u0DFF\\u200a-\\u200d]')\r\n",
    "train_file = \"dataset/Other/valid8.txt\"\r\n",
    "train_corpus_2 = get_words(train_file, '[^\\u0D80-\\u0DFF\\u200a-\\u200d]')\r\n",
    "\r\n",
    "output = []\r\n",
    "\r\n",
    "def compute_metrics(name, corpus, train_corpus):\r\n",
    "    return {\r\n",
    "    \"Courpus Name\" : name,\r\n",
    "    \"Word Count\" : len(corpus),\r\n",
    "    \"Unique Word Count\" : len(set(corpus)),\r\n",
    "    \"Unique Words intersecting with Trainning Corpus\" : len(set(corpus).intersection(train_corpus))}\r\n",
    "\r\n",
    "output.append(compute_metrics(\"Train_0\", train_corpus_0, vocab))\r\n",
    "output.append(compute_metrics(\"Train_1\", train_corpus_1, vocab))\r\n",
    "output.append(compute_metrics(\"Train_2\", train_corpus_2, vocab))\r\n",
    "output.append(compute_metrics(\"Test\", test_corpus, vocab))\r\n",
    "for i, key in enumerate(isolated_test_cases.keys()):\r\n",
    "    output.append(compute_metrics(str(i), isolated_test_cases[key], vocab))\r\n",
    "\r\n",
    "# write the computed results\r\n",
    "csv_file = 'corpus_evaluations.csv'\r\n",
    "csv_file =  open(csv_file, mode= 'w+' ,newline='', encoding='utf-8')\r\n",
    "writer = csv.DictWriter(csv_file, fieldnames=list(output[0].keys()))\r\n",
    "writer.writeheader()\r\n",
    "for record in output:\r\n",
    "    writer.writerow(record)\r\n",
    "csv_file.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from preProcessing import get_words\r\n",
    "train_file = \"dataset/train_06_jun_good_with_articles.txt\"\r\n",
    "w0 = get_words(train_file, '[^\\u0D80-\\u0DFF\\u200a-\\u200d]')\r\n",
    "train_file = \"dataset/train_06_jun_good.txt\"\r\n",
    "w1 = get_words(train_file, '[^\\u0D80-\\u0DFF\\u200a-\\u200d]')\r\n",
    "\r\n",
    "print(len(w0), len(w1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# outputfile\r\n",
    "import os\r\n",
    "import re\r\n",
    "\r\n",
    "ofile = \"dataset/train/combined.txt\"\r\n",
    "folders = [\r\n",
    "    'dataset/train/text_books'\r\n",
    "] \r\n",
    "def file_extractor(folder_path_lst, ofile):\r\n",
    "    f_out =  open(ofile, mode= 'w', encoding='utf8')\r\n",
    "    for folder_path in folder_path_lst:\r\n",
    "        for path, subdirs, files in os.walk(folder_path):\r\n",
    "            for f in files: \r\n",
    "                if '.txt' == f[-4:]:\r\n",
    "                    with open(os.path.join(path, f), mode= 'r', encoding='utf8',  errors='ignore') as f:\r\n",
    "                        f_out.writelines(f.readlines())\r\n",
    "    f_out.close()\r\n",
    "\r\n",
    "file_extractor(folders, ofile)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# outputfile\r\n",
    "import os\r\n",
    "import re\r\n",
    "\r\n",
    "ofile = \"dataset/train/combined_articles.txt\"\r\n",
    "folders = [\r\n",
    "    'dataset/train/14_JUN'\r\n",
    "] \r\n",
    "\r\n",
    "\r\n",
    "file_extractor(folders, ofile)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# outputfile\r\n",
    "import os\r\n",
    "import re\r\n",
    "from preProcessing import unicode_error_correction, encode_error_correction\r\n",
    "\r\n",
    "ofile = \"dataset/train/text_book.txt\"\r\n",
    "folders = [\r\n",
    "    'dataset/train/text_books/test_book'\r\n",
    "] \r\n",
    "\r\n",
    "def file_extractor(folder_path_lst, ofile):\r\n",
    "    f_out =  open(ofile, mode= 'w', encoding='utf8')\r\n",
    "    for folder_path in folder_path_lst:\r\n",
    "        for path, subdirs, files in os.walk(folder_path):\r\n",
    "            for f in files: \r\n",
    "                if '.txt' == f[-4:]:\r\n",
    "                    with open(os.path.join(path, f), mode= 'r', encoding='utf8',  errors='ignore') as f:\r\n",
    "                        f_out.writelines([line for line in f.readlines()])\r\n",
    "                        # f_out.writelines([encode_error_correction(line) for line in f.readlines()])\r\n",
    "    f_out.close()\r\n",
    "\r\n",
    "file_extractor(folders, ofile)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ofile = \"dataset/train/10_JUN/raw_data.txt\"\r\n",
    "file_path = [\r\n",
    "    \"dataset/train/24_MAY/TrainedDataRemoved.txt\",\r\n",
    "    \"dataset/train/10_JUN/ManuallyAddedData.txt\",\r\n",
    "    \"dataset/train/combined.txt\",\r\n",
    "    \"dataset/train/combined_articles.txt\",\r\n",
    "    # not 100% correct\r\n",
    "    \"dataset/train/lankadeepa_corpus.txt\",\r\n",
    "    \"dataset/train/text_book.txt\",\r\n",
    "    # \"dataset/Other/train8.txt\"\r\n",
    "]\r\n",
    "f_out =  open(ofile, mode= 'w', encoding='utf8')\r\n",
    "for f in file_path:\r\n",
    "    with open(os.path.join(f), mode= 'r', encoding='utf8', errors='ignore') as f:\r\n",
    "        f_out.writelines(set(f.readlines()))\r\n",
    "f_out.close()\r\n",
    "\r\n",
    "# deduplications\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# run this script to generate cleaned data from raw data\r\n",
    "\r\n",
    "from preProcessing import *\r\n",
    "from preProcessing import correct_words\r\n",
    "import json\r\n",
    "file_path = 'dataset/train/10_JUN/raw_data.txt'\r\n",
    "clean0_path = 'dataset/PreProcessing/cleaning.json'\r\n",
    "with open(file_path, mode='r', encoding='utf8') as f:\r\n",
    "    text = f.read()\r\n",
    "with open(clean0_path, mode='r', encoding='utf8') as f:\r\n",
    "    rules_dict = json.load(f)\r\n",
    "    rules_dict = {k : v for k,v in rules_dict.items() if v != None}\r\n",
    "text = unicode_error_correction(text, False)\r\n",
    "text = correct_words(text, rules_dict)\r\n",
    "file_path = 'dataset/train/10_JUN/ucleaned_data.txt'\r\n",
    "with open(file_path, mode='w+', encoding='utf8') as f:\r\n",
    "    f.write(text)\r\n",
    "from scRNN.error_word_generation import AddNoise\r\n",
    "import json\r\n",
    "file_path  = 'dataset/train/10_JUN/ucleaned_data.txt'\r\n",
    "AddNoise_obj = AddNoise.getAddNoiseObj(file_path)\r\n",
    "AddNoise_obj.addNoiseFile('dataset/preprocessed')\r\n",
    "\r\n",
    "with open('vocab_jun10.json', mode= 'w+', encoding='utf8') as f:\r\n",
    "     json.dump({k :v for k, v in sorted(AddNoise_obj.freq().items(), key=lambda item: item[0])}, f, indent= 4, ensure_ascii=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from preProcessing import *\r\n",
    "from preProcessing import correct_words\r\n",
    "import json\r\n",
    "file_path = 'dataset\\Other\\data_ctest_good.txt'\r\n",
    "clean0_path = 'dataset/PreProcessing/cleaning.json'\r\n",
    "with open(file_path, mode='r', encoding='utf8') as f:\r\n",
    "    text = f.read()\r\n",
    "with open(clean0_path, mode='r', encoding='utf8') as f:\r\n",
    "    rules_dict = json.load(f)\r\n",
    "    rules_dict = {k : v for k,v in rules_dict.items() if v != None}\r\n",
    "text = unicode_error_correction(text, False)\r\n",
    "text = correct_words(text, rules_dict)\r\n",
    "file_path = 'dataset\\Other\\data_ctest_good_cleaned.txt'\r\n",
    "with open(file_path, mode='w+', encoding='utf8') as f:\r\n",
    "    f.write(text)\r\n",
    "from scRNN.error_word_generation import AddNoise\r\n",
    "import json\r\n",
    "file_path  = 'dataset\\Other\\data_ctest_good_cleaned.txt'\r\n",
    "AddNoise_obj = AddNoise.getAddNoiseObj(file_path)\r\n",
    "AddNoise_obj.addNoiseFile('dataset\\Other')\r\n",
    "\r\n",
    "with open('vocab_jun10.json', mode= 'w+', encoding='utf8') as f:\r\n",
    "     json.dump({k :v for k, v in sorted(AddNoise_obj.freq().items(), key=lambda item: item[0])}, f, indent= 4, ensure_ascii=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "creating word dictionary\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2037/2037 [00:01<00:00, 1656.72it/s]\n",
      "100%|██████████| 8557/8557 [00:00<00:00, 2140042.89it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scRNN.error_word_generation import generateErrorsWithDictionary\r\n",
    "import json\r\n",
    "\r\n",
    "file_path = 'dataset/train/silumina/silumina_corpus_text_corrected.txt'\r\n",
    "# file_path = 'dataset/Resources/VerifiedUniqueWordList-Sinhala/parallel-08.11.2020-Tr74K.si-en-si_unique.txt'\r\n",
    "error_lookup = 'ErrorLookUp.json'\r\n",
    "with open(error_lookup, mode='r', encoding='utf8') as f:\r\n",
    "    error_lookup_1 = json.load(f)\r\n",
    "\r\n",
    "AddNoise_obj = generateErrorsWithDictionary(file_path, error_lookup_1)\r\n",
    "AddNoise_obj.addNoiseFile('dataset/train/24JUL')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\r\n",
    "from scRNN.error_word_generation import generateErrorsWithDictionary\r\n",
    "import json\r\n",
    "\r\n",
    "file_path = 'dataset/preprocessed/train_06_jun_good.txt'\r\n",
    "error_lookup = 'ErrorLookUp.json'\r\n",
    "with open(error_lookup, mode='r', encoding='utf8') as f:\r\n",
    "    error_lookup_1 = json.load(f)\r\n",
    "\r\n",
    "AddNoise_obj = generateErrorsWithDictionary(file_path, error_lookup_1)\r\n",
    "AddNoise_obj.addNoiseFile('dataset/train/24JUL/06AUG')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 454206/454206 [00:46<00:00, 9820.80it/s] \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_path = 'dataset/train/silumina/silumina_corpus_text_corrected.txt'\r\n",
    "with open(file_path, mode='r+', encoding='utf8') as f_in:\r\n",
    "    print('Good ', len(f_in.readlines()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\r\n",
    "def load_data(base_path, corr_file, incorr_file):\r\n",
    "    \r\n",
    "    # load files\r\n",
    "    if base_path:\r\n",
    "        assert os.path.exists(base_path)==True, base_path\r\n",
    "    incorr_data = []\r\n",
    "    opfile1 = open(os.path.join(base_path, incorr_file),\"r\")\r\n",
    "    for line in opfile1:\r\n",
    "        if line.strip()!=\"\": incorr_data.append(line.strip())\r\n",
    "        else:print(len(incorr_data), line)\r\n",
    "    opfile1.close()\r\n",
    "    corr_data = []\r\n",
    "    opfile2 = open(os.path.join(base_path, corr_file),\"r\")\r\n",
    "    for line in opfile2:\r\n",
    "        if line.strip()!=\"\": corr_data.append(line.strip())\r\n",
    "        else:print(len(corr_data), line)\r\n",
    "    opfile2.close()\r\n",
    "    assert len(incorr_data)==len(corr_data), \"{} {}\".format(len(incorr_data), len(corr_data))\r\n",
    "    \r\n",
    "    # verify if token split is same\r\n",
    "    for i,(x,y) in tqdm(enumerate(zip(corr_data,incorr_data))):\r\n",
    "        x_split, y_split = x.split(), y.split()\r\n",
    "        try:\r\n",
    "            assert len(x_split)==len(y_split)\r\n",
    "        except AssertionError:\r\n",
    "            print(\"# tokens in corr and incorr mismatch. retaining and trimming to min len.\")\r\n",
    "            print(x_split, y_split)\r\n",
    "            mn = min([len(x_split),len(y_split)])\r\n",
    "            corr_data[i] = \" \".join(x_split[:mn])\r\n",
    "            incorr_data[i] = \" \".join(y_split[:mn])\r\n",
    "            print(corr_data[i],incorr_data[i])\r\n",
    "    \r\n",
    "    # return as pairs\r\n",
    "    data = []\r\n",
    "    for x,y in tqdm(zip(corr_data,incorr_data)):\r\n",
    "        data.append((x,y))\r\n",
    "    \r\n",
    "    print(f\"loaded tuples of (corr,incorr) examples from {base_path}\")\r\n",
    "    return data\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "load_data('dataset/train/24JUL', 'train_06_jun_bad.txt', 'train_06_jun_good.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\r\n",
    "from preProcessing import correct_words \r\n",
    "token_freq = {}\r\n",
    "with open('ErrorLookUp.json', mode='r+', encoding='utf8') as f:\r\n",
    "    ErrorLookUp = json.load(f)\r\n",
    "with open(\"C:/Users/chara/Documents/Work/RE_CS/neuspell/silumina_all.json\", mode='r+', encoding='utf8') as f:\r\n",
    "    tokens = json.load(f)[\"token2idx\"]\r\n",
    "with open(\"C:/Users/chara/Documents/Work/RE_CS/neuspell/silumina_all.json\", mode='r+', encoding='utf8') as f:\r\n",
    "    for item in json.load(f)[\"token_freq\"]:\r\n",
    "        token_freq[item[0]] = item[1]\r\n",
    "\r\n",
    "with open(\"dataset/Resources/dictionary.json\", mode='r+', encoding='utf8') as f:\r\n",
    "    dictionary = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "### find natural encoding issues\r\n",
    "\r\n",
    "import sqlite3\r\n",
    "from preProcessing import encoding_error_at_start\r\n",
    "\r\n",
    "recs_per_time = 1000\r\n",
    "max_limit = 2\r\n",
    "conn = sqlite3.connect('ErrorLookUp.db')\r\n",
    "freq_based_correction = {}\r\n",
    "num_of_records = 0\r\n",
    "# get number of records\r\n",
    "table_name = 'ErrorLookUp5'\r\n",
    "cursor = conn.execute('''SELECT COUNT(*)\r\n",
    "                         FROM {};'''.format(table_name))\r\n",
    "for row in cursor:\r\n",
    "    num_of_records = row[0]\r\n",
    "for i in range(0, num_of_records, recs_per_time):\r\n",
    "    cursor = conn.execute('''SELECT *\r\n",
    "                             FROM {}\r\n",
    "                             LIMIT {} OFFSET {} ;'''.format(table_name, recs_per_time, i))\r\n",
    "    for row in cursor:\r\n",
    "        word, perm_no, lst = row\r\n",
    "        lst = lst.split(',')\r\n",
    "        if isinstance(lst, str):\r\n",
    "            lst = [lst]\r\n",
    "        # print(word, perm_no, lst)\r\n",
    "        # print(type(word), type(perm_no), type(lst))\r\n",
    "        if perm_no <= max_limit and perm_no != 0:\r\n",
    "            for word_i in lst:\r\n",
    "                cursor_i = conn.execute('''SELECT *\r\n",
    "                                        FROM {}\r\n",
    "                                        WHERE WORD =='{}' AND PERM_NUM == 0;'''.format(table_name, word_i))\r\n",
    "                for row_i in cursor_i:\r\n",
    "                    # print(word, row_i)\r\n",
    "                    error_word = word_i\r\n",
    "                    k = word\r\n",
    "                    if error_word in dictionary and k in dictionary or token_freq[error_word] == token_freq[k]:\r\n",
    "                        continue\r\n",
    "                    if token_freq[error_word] > token_freq[k] or error_word in dictionary: \r\n",
    "                        corr = error_word\r\n",
    "                        incorr = k\r\n",
    "                        freq_based_correction[incorr] = corr\r\n",
    "                    # elif token_freq[k] > token_freq[error_word]:\r\n",
    "                    #     corr = k\r\n",
    "                    #     incorr = error_word\r\n",
    "                    #     freq_based_correction[incorr] = corr\r\n",
    "                    else:\r\n",
    "                        # print(error_word, k)\r\n",
    "                        pass\r\n",
    "    # break\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(freq_based_correction.items())\r\n",
    "with open(\"freq_corrections_5.json\", mode='w', encoding='utf8') as f:\r\n",
    "    json.dump({k : v for k,v in sorted(freq_based_correction.items())}, f, ensure_ascii=False, indent= 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# adding errors to a table\r\n",
    "try:\r\n",
    "    conn.execute('''CREATE TABLE NaturalErrors\r\n",
    "            (Error_Word TEXT NOT NULL,\r\n",
    "            Correct_Word TEXT NOT NULL);''')\r\n",
    "except sqlite3.OperationalError:\r\n",
    "    conn.execute('''DROP TABLE NaturalErrors''')\r\n",
    "    conn.commit()\r\n",
    "    print(\"Table Dropped\")\r\n",
    "    conn.execute('''CREATE TABLE NaturalErrors\r\n",
    "            (Error_Word TEXT NOT NULL,\r\n",
    "            Correct_Word TEXT NOT NULL);''')\r\n",
    "for error,correct in freq_based_correction.items():\r\n",
    "    conn.execute('''INSERT INTO NaturalErrors (Error_Word, Correct_Word) VALUES ('{}', '{}')'''.format(error,correct))\r\n",
    "    conn.commit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Table Dropped\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "conn = sqlite3.connect('ErrorLookUp.db')\r\n",
    "incorr = 'උතු‍ෙර්'\r\n",
    "corr = 'උතුරේ'\r\n",
    "conn.execute('''INSERT INTO NaturalErrors (Error_Word, Correct_Word) VALUES ('{}', '{}')'''.format(incorr,corr))\r\n",
    "conn.commit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "cursor = conn.execute('''SELECT COUNT(*)\r\n",
    "                         FROM NaturalErrors;''')\r\n",
    "freq_based_correction = {}\r\n",
    "for row in cursor:\r\n",
    "    num_of_records = row[0]\r\n",
    "for i in range(0, num_of_records, recs_per_time):\r\n",
    "    cursor = conn.execute('''SELECT *\r\n",
    "                             FROM NaturalErrors\r\n",
    "                             LIMIT {} OFFSET {} ;'''.format(recs_per_time, i))\r\n",
    "    for row in cursor:\r\n",
    "        error, correct = row\r\n",
    "        freq_based_correction[error] = correct"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "len(freq_based_correction.items())\r\n",
    "with open(\"freq_corrections.json\", mode='w', encoding='utf8') as f:\r\n",
    "    json.dump(freq_based_correction, f, ensure_ascii=False, indent= 4)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}