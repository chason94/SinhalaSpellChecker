# -*- coding: utf-8 -*-
"""LocalModelPerfromanceEvalution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QgK2CBjNA993lqKQR98Qk4XFUWulrMlq
"""

import argparse
import datetime
import numpy as np
import csv
import os
import json
import re
import tensorflow as tf
from SinhalaSpellChecker.scRNN import DataLoaderMode, DataLoader, \
 Sinhala_alpha, symbols, NoiseType, get_words, colors, decode_word, vector_type
from keras.models import Sequential
import keras
from keras.layers import Activation, Dense, Dropout, LSTM, TimeDistributed
from sklearn.model_selection import train_test_split
from keras.models import load_model



path = "results"
n_epoch = 20  # number of epochs
n_units = 650  # number of units per layer
batchsize = 20  # minibatch size
check_point = 10  # checkpoint (num epoch)
vocab_path = os.path.join(path,'train_c_corpus+text.json')
EXP_NAME = "train_c_corpus+text_units_650_batch_20_NS_NAT_SINHALA_Sparse" #corpus_textbook_00_train_units_650_batch_20_NATURAL
checkpoint_filepath = os.path.join(path, EXP_NAME)

with open(vocab_path, 'r',encoding='utf8') as f:
  vocab = json.load(f)
id2vocab = {value : key for key, value in vocab.items()}

data_dim = (len(Sinhala_alpha) + len(symbols)) * 3

print(list(vocab.items())[-10:])
print(vocab['<unk>'])

len(vocab.items())
model = tf.keras.models.load_model(checkpoint_filepath)
model.summary()




def evaluate(PATH_NATURAL_TEST_BAD, PATH_NATURAL_TEST_GOOD, EXP_NAME):
  test = DataLoader(  PATH_NATURAL_TEST_BAD, 
                      PATH_NATURAL_TEST_GOOD, 
                      batch_size=batchsize,
                      x_dim=data_dim ,
                      vocab=vocab ,
                      noise=noise_type,
                      alpha=Sinhala_alpha + symbols,
                      mode=DataLoaderMode.test ,
                      vector_type =vector_type.sinhala ,
                    )

  print_count = 0 
  for j in range(len(test)):
    _, _, x_token, y_token = test.get_list_for_testing(j) 
    if x_token != y_token : 
      for k in range(len(x_token)):
        if x_token[k] == y_token[k]:
            x_token[k] = colors(x_token[k], color = 'green')
        if x_token[k] != y_token[k]:
            x_token[k] = colors(x_token[k], color='red')

      src_j = " ".join(x_token)
      ref_j = " ".join(y_token) 

      print("src_j :{}".format(src_j))
      print("ref_j :{}".format(ref_j))
      if print_count > 10 : 
        del print_count
        break
      print_count += 1

  total_pred_j_list = []
  total_ref_j_list = []
  total_src_j_list = []

  for j in range(len(test)):
      x_raw, _, x_token, y_token = test.get_list_for_testing(j) 
      src_j = " ".join(x_token)
      ref_j = " ".join(y_token) 
      preds = model.predict(x_raw, verbose=0)
      pred_j = decode_word(preds[0], calc_argmax=True, id2vocab = id2vocab)


      ref_j_list = ref_j.split()
      pred_j_list = pred_j.split()
      src_j_list = src_j.split() 

      # to get total lists
      total_pred_j_list.append(pred_j_list.copy())
      total_ref_j_list.append(ref_j_list.copy())
      total_src_j_list.append(src_j_list.copy())


      
      with open(EXP_NAME + 'predicted_text.txt', 'a', encoding='utf8') as f :
        f.write('example # {}\n'.format(str(j + 1)))
        f.write('src: {}\n'.format( src_j))
        f.write('prd: {}\n'.format(pred_j))
        f.write('ref: {}\n'.format(ref_j))

      # coloring

      for k in range(len(pred_j_list)):
          if pred_j_list[k] == ref_j_list[k]:
              pred_j_list[k] = colors(pred_j_list[k])
          if src_j_list[k] != ref_j_list[k]:
              src_j_list[k] = colors(src_j_list[k],color='red')
      pred_j = " ".join(pred_j_list)
      src_j = " ".join(src_j_list)
      # extra
      pred_j = " ".join(pred_j_list)

      # if j < 50 and src_j != ref_j:
      #   print('example # {}\n'.format(str(j + 1)))
      #   print('src: {}\n'.format(src_j))
      #   print('prd: {}\n'.format(pred_j))
      #   print('ref: {}\n'.format(ref_j))

  # consider only the words in the vocab
  # results

  def safe_division(n, d):
      return n / d if d > 0 else 0


  pred_edits_ps = total_pred_j_list
  gold_edits_ps = total_ref_j_list
  x_ps = total_src_j_list

  print(pred_edits_ps[0], gold_edits_ps[0], x_ps[0])

  gold_tps = []
  pred_tps = []

  d_tp, d_tn, d_fp, d_fn = 0, 0, 0, 0  # detection
  c_tp, c_tn, c_fp, c_fn = 0, 0, 0, 0  # correction

  for i, (x, gold, pred) in enumerate(zip(x_ps, gold_edits_ps, pred_edits_ps)):
      # print(x,gold,pred)
      
      for o_word, g_word, e_word in zip(x, gold, pred):
        if g_word in [".", "<eos>"]:
          continue
        o_word = o_word.strip()
        g_word = g_word.strip()
        e_word = e_word.strip()

        if o_word == g_word:  # no error
            if o_word == e_word:
                d_tn += 1  # no error, not detected an error
            else:
                d_fp += 1  # no error, but detected an error

            if g_word == e_word:
                c_tn += 1  # no error, not corrected
            else:
                c_fp += 1  # no error, but corrected
        else:  # error; o_word != g_word
            if o_word == e_word:
                d_fn += 1  # error, but not detected an error
            else:
                d_tp += 1  # error, detected an error

            if g_word == e_word:
                c_tp += 1  # error, corrected accurately
            else:
                c_fn += 1  # error, not/inaccurately corrected

  d_recall = safe_division(d_tp, d_tp + d_fn)
  d_prec = safe_division(d_tp, d_fp + d_tp)
  c_recall = safe_division(c_tp, c_tp + c_fn)
  c_prec = safe_division(c_tp, c_fp + c_tp)

  detection_accuracy = safe_division((d_tn + d_tp), (d_tn + d_tp + d_fn + d_fp)) * 100
  detection_F1 = safe_division((2 * d_recall * d_prec * 100), (d_recall + d_prec))
  detection_F1 = safe_division((2 * d_recall * d_prec * 100), (d_recall + d_prec))
  corrections_accuracy =  safe_division((c_tn + c_tp), (c_tn + c_tp + c_fn + c_fp)) * 100
  corrections_F1 =  safe_division((2 * c_recall * c_prec * 100), (c_recall + c_prec))
  corrections_F1 =  safe_division((2 * c_recall * c_prec * 100), (c_recall + c_prec))

  with open(EXP_NAME + 'consfusion_results.txt', 'w+', encoding="utf8") as file:
    print('Confusion matrix based evaluation :- \n\n', d_tp+d_tn+d_fp+d_fn, file=file)
    print('Detection TP {} \n'.format(d_tp), file=file)
    print('Detection TN {} \n'.format(d_tn), file=file)
    print('Detection FP {} \n'.format(d_fp), file=file)
    print('Detection FN {} \n'.format(d_fn), file=file)
    print('Detection Accuracy: {}% \n'.format(detection_accuracy), file=file)
    print('Detection Recall: {}% \n'.format(d_recall * 100), file=file)
    print('Detection Precision: {}% \n'.format(d_prec * 100), file=file)
    print('Detection F1 {} \n'.format(detection_F1), file=file)
    print('{} \n'.format('=' * 20), file=file)
    print('Correction TP {} \n'.format(c_tp), file=file)
    print('Correction TN {} \n'.format(c_tn), file=file)
    print('Correction FP {} \n'.format(c_fp), file=file)
    print('Correction FN {} \n'.format(c_fn), file=file)
    print('Correction Accuracy: {}% \n'.format(corrections_accuracy), file=file)
    print('Correction Recall: {}% \n'.format(c_recall * 100), file=file)
    print('Correction Precision: {}% \n'.format(c_prec * 100), file=file)
    print('Correction F1 {} \n'.format(corrections_F1), file=file)

  

PATH_TEST = "test"
noise_type = NoiseType.NONE
PATH_NATURAL_TEST_BAD = ['dataset/eof_cdata_test_bad.txt'] 
PATH_NATURAL_TEST_GOOD = ['dataset/eof_cdata_test_good.txt']

files_to_test = os.listdir('dataset/test_00_good')
PATH_NATURAL_TEST_BAD.extend([os.path.join('dataset/test_00_bad', f) for f in  sorted(files_to_test)])
PATH_NATURAL_TEST_GOOD.extend([os.path.join('dataset/test_00_good', f) for f in  sorted(files_to_test)])

for bad, good in zip(PATH_NATURAL_TEST_BAD,PATH_NATURAL_TEST_GOOD):
  EXP_NAME = os.path.join(PATH_TEST, good.split('/')[-1])
  evaluate(bad, good, EXP_NAME)
