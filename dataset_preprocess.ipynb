{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0dfd7741fd10dbdafac6de92b706d283abdbabb2e94c221243fc4b09e345fee76",
   "display_name": "Python 3.8.8 64-bit ('SinhalaSpellChecker': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "from scRNN import Sinhala_alpha, get_words\n",
    "from scRNN.constants import NUM\n",
    "from scRNN.error_word_generation import GenErrorWords, ErrorType\n",
    "from preProcessing import read_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path =  \"dataset\"\n",
    "log_file = \"log.txt\"\n",
    "if os.path.exists(os.path.join(dataset_path, log_file)):\n",
    "    os.remove(os.path.join(dataset_path, log_file))\n",
    "output_path = os.path.join(dataset_path, \"preprocessed\") \n",
    "new_corp = 'dataset/train/24_MAY/TrainedDataRemoved.txt'\n",
    "text_book = 'dataset/train/data_from_text_books/text_book.txt'\n",
    "data_good = \"dataset/Other/eof_data_good.txt\"\n",
    "data_bad = \"dataset/Other/eof_data_bad.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corp_sentences = read_sentences(new_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean stage 01\n",
    "# replace english words with <unk>\n",
    "# replace number with NUM defined on constants\n",
    "def clean_stage_01(list_senteces):\n",
    "    rlist_senteces = []\n",
    "    for sentence in list_senteces:\n",
    "        rword_lst = []\n",
    "        word_lst = sentence.split()\n",
    "        for word in word_lst:\n",
    "            if word in ['.', '<eos>', '<unk>']:\n",
    "                rword_lst.append(word)\n",
    "                continue\n",
    "            # word = re.sub(r'[A-z]+', '<unk>', word)\n",
    "            # word = re.sub(r'[0-9,]+', NUM, word)\n",
    "            rword_lst.append(word)\n",
    "        rlist_senteces.append(\" \".join(rword_lst))\n",
    "    return rlist_senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_corp_sentences = clean_stage_01(new_corp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean stage 02\n",
    "# replace english words with <unk>\n",
    "# replace number with NUM defined on constants\n",
    "def clean_stage_02(list_senteces):\n",
    "    rlist_senteces = []\n",
    "    with open(os.path.join(dataset_path, log_file), mode='a', encoding='utf8') as f:\n",
    "        for sentence in list_senteces:\n",
    "            rword_lst = []\n",
    "            word_lst = sentence.split()\n",
    "            for word in word_lst:\n",
    "                if word in ['.', '<eos>', '<unk>']:\n",
    "                    rword_lst.append(word)\n",
    "                    continue\n",
    "                word_prev = word\n",
    "                word = re.sub('\\u200d්','්', word) \n",
    "                word = re.sub(r'අා', 'ආ', word)\n",
    "                word = re.sub(r'අැ', 'ඇ', word)\n",
    "                word = re.sub(r'ේ', 'ේ', word)\n",
    "                word = re.sub(r'ෙෙ','ෛ', word)\n",
    "                word = re.sub(r'ෝ', 'ෝ', word)\n",
    "                word = re.sub(r'ෝ', 'ෝ', word)\n",
    "                word = re.sub(r'ෘෘ', 'ෲ', word)\n",
    "                word = re.sub(r'[<>˚ை>\\\\/”{}¼½⅓’‘”‚.‟÷–\\]]',' ', word)\n",
    "                if word_prev != word:\n",
    "                    print(\"{} -> {}\".format(word_prev, word), file = f)\n",
    "                    print(\"{} -> {}\".format(list(word_prev), list(word)), file = f)\n",
    "                rword_lst.append(word)\n",
    "            rlist_senteces.append(\" \".join(rword_lst))\n",
    "        print(\"-\"*40 + \"\\n\", file =f)\n",
    "    \n",
    "    return rlist_senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_corp_sentences = clean_stage_02(new_corp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of sentences 96665\nnumber of sentences 96665\nnumber of sentences 62295\n"
     ]
    }
   ],
   "source": [
    "print(\"number of sentences {}\".format(len(new_corp_sentences)))\n",
    "corpus_senteces = new_corp_sentences #+ text_book_sentences\n",
    "print(\"number of sentences {}\".format(len(corpus_senteces)))\n",
    "corpus_senteces = set(corpus_senteces) #\n",
    "print(\"number of sentences {}\".format(len(corpus_senteces)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(file_to_write, sentence_list):\n",
    "    with open(file_to_write, 'w+', encoding='utf8') as f:\n",
    "        f.writelines(\"\\n\".join(sentence_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of words in training 35576\n"
     ]
    }
   ],
   "source": [
    "# write to file\n",
    "file_to_write = os.path.join(output_path, 'train_corpus_y.txt') \n",
    "to_file(file_to_write, corpus_senteces)\n",
    "words_train = get_words(file_to_write)\n",
    "print(\"number of words in training {}\".format(len(words_train)))\n",
    "del words_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# data to be used and primary test set and dev set\n",
    "PATH_DEV_GOOD_SOURCE = 'dataset/test_01/eof_data_test_good.txt'\n",
    "PATH_DEV_GOOD_RESULT = 'dataset/test_01/eof_cdata_test_good.txt'\n",
    "\n",
    "data_dev = read_sentences(PATH_DEV_GOOD_SOURCE)\n",
    "data_dev = clean_stage_01(data_dev)\n",
    "data_dev = clean_stage_02(data_dev)\n",
    "to_file(PATH_DEV_GOOD_RESULT, data_dev)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 323,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TEST_GOOD_SOURCE = 'dataset/test_01/eof_data_test_bad.txt'\n",
    "PATH_TEST_GOOD_RESULT = 'dataset/test_01/eof_cdata_test_bad.txt'\n",
    "\n",
    "data_TEST = read_sentences(PATH_TEST_GOOD_SOURCE)\n",
    "data_TEST = clean_stage_01(data_TEST)\n",
    "data_TEST = clean_stage_02(data_TEST)\n",
    "to_file(PATH_TEST_GOOD_RESULT, data_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to be used and primary test set and dev set\n",
    "PATH_TEST_GOOD_SOURCE = 'dataset/test_raw/'\n",
    "PATH_TEST_GOOD_RESULT = 'dataset/test_00_good/'\n",
    "\n",
    "for file in os.listdir(PATH_TEST_GOOD_SOURCE):\n",
    "    if 'eof_doc' in file:\n",
    "        data_dev = read_sentences(PATH_TEST_GOOD_SOURCE+file)\n",
    "        data_dev = clean_stage_01(data_dev)\n",
    "        data_dev = clean_stage_02(data_dev)\n",
    "        file_to_write = PATH_TEST_GOOD_RESULT + \"c_\" + \"_\".join(file.split(\"_\")[1:]) \n",
    "        to_file(file_to_write, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN_GOOD = os.path.join(output_path, 'train_corpus_y.txt') \n",
    "PATH_TEST = 'dataset/test_00_good'\n",
    "PATH_DEV_GOOD = 'dataset/test_01/eof_cdata_test_good.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of words in training 29137\nnumber of words in dev 8508\nnumber of words in test 1816\ntotal vocab size 33225\n"
     ]
    }
   ],
   "source": [
    "# load vocab in train\n",
    "words_train = get_words(PATH_TRAIN_GOOD)\n",
    "print(\"number of words in training {}\".format(len(words_train)))\n",
    "# load vocab in dev\n",
    "words_dev = get_words(PATH_DEV_GOOD)\n",
    "print(\"number of words in dev {}\".format(len(words_dev)))\n",
    "# load vocan in test\n",
    "words_test = []\n",
    "for file in os.listdir(PATH_TEST):\n",
    "    words_test.extend(get_words(os.path.join(PATH_TEST, file)))\n",
    "print(\"number of words in test {}\".format(len(words_test)))\n",
    "vocab = {k : i for i, k in enumerate(set(words_train + words_dev + words_test))}\n",
    "if \"<eos>\" not in vocab.keys():\n",
    "    vocab[\"<eos>\"] = len(vocab.keys()) - 1\n",
    "if \"<unk>\" not in vocab.keys():\n",
    "    vocab[\"<unk>\"] = len(vocab.keys()) - 1\n",
    "\n",
    "print(\"total vocab size {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of words common in train & dev 4850\nnumber of words common in train & test 993\n"
     ]
    }
   ],
   "source": [
    "print(\"number of words common in train & dev {}\".format(len(set(words_train).intersection(set(words_dev)))))\n",
    "print(\"number of words common in train & test {}\".format(len(set(words_train).intersection(set(words_test)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_vocab.txt', mode= 'w', encoding='utf8') as f:\n",
    "    f.write(\"\\n\".join(sorted(vocab.keys(), key=lambda l: len(l), reverse=True)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "with open('train_c_corpus+text.json', mode= 'r', encoding='utf8') as f:\n",
    "    vocab_from_file = json.load(f)\n",
    "type(vocab_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_add(file_in, file_out, dir_name):\n",
    "    dir_name = dir_name\n",
    "    file_name = os.path.join(file_in)\n",
    "    out_file_name = os.path.join(dir_name, file_out)\n",
    "\n",
    "    logging.basicConfig(handlers=[logging.FileHandler(os.path.join(dir_name, \"log_\"+file_in), 'w', 'utf-8')],\n",
    "                        level=logging.DEBUG)\n",
    "\n",
    "    error_types = {  # replace maximum of two chars with\n",
    "        ErrorType.Replace: [1, 2],\n",
    "        ErrorType.Delete: [1, 1],\n",
    "        ErrorType.Insert: [1, 1]\n",
    "    }\n",
    "    gne = GenErrorWords(random_seed= 1, #random.randint(0,99),\n",
    "                        error_types=error_types,\n",
    "                        vocab=vocab_from_file)\n",
    "    input_file_name = os.path.join(dir_name, file_name)\n",
    "    with open(input_file_name, 'r+', encoding='utf8')as f:\n",
    "        file_data = f.read().strip().split()\n",
    "\n",
    "    print(out_file_name)\n",
    "    GenErrorWords.add_noise_to_train(\n",
    "        filedata=file_data,\n",
    "        Obj=gne, \n",
    "        outfile_name=out_file_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output_bad.txt\nlogs written to output_bad_log.txt\n\n"
     ]
    }
   ],
   "source": [
    "noise_add(\"output.txt\", \"output_bad.txt\", '')\n",
    "# noise_add(\"eof_cdata_test_good.txt\", \"eof_cdata_dev_bad.txt\", 'dataset/test_01')\n",
    "# noise_add(\"eof_cdata_test_good.txt\", \"eof_cdata_test_bad.txt\", 'dataset/test_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append good data and bad data\n",
    "# with open(\"dataset/train_corpus_y.txt\", 'a+', encoding='utf8') as f:\n",
    "#         f.writelines(\"\\n\".join(data_good))\n",
    "# with open(\"dataset/train_corpus_x.txt\", 'a+', encoding='utf8') as f:\n",
    "#         f.writelines(\"\\n\".join(data_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words(file_name):\n",
    "    with open(file_name, 'r+', encoding='utf8') as f:\n",
    "        return f.read().split()\n",
    "y = all_words(\"dataset/train_corpus_y.txt\")\n",
    "x = all_words(\"dataset/train_corpus_x.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_words(file_name):\n",
    "    with open(file_name, 'r+', encoding='utf8') as f:\n",
    "        return f.read().split()\n",
    "y = all_words(\"train_corpus_y_00.txt\")\n",
    "word_freq = {k : 0  for k,v in vocab.items()}\n",
    "word_freq['unk'] = 0\n",
    "for word in y :\n",
    "    if word in word_freq.keys():\n",
    "        word_freq[word] += 1\n",
    "    else:\n",
    "        word_freq[word] = 1\n",
    "freq = list(sorted(word_freq.items(), key=lambda x: x[1]))\n",
    "# freq[-100:]\n",
    "bins = 20\n",
    "freq.pop(-1)\n",
    "freq.pop(-1)\n",
    "freq[-100:]\n",
    "max_val = freq[-1][1]\n",
    "min_val = 0\n",
    "binsep = [i for i in range(min_val, max_val, int((max_val-min_val)/bins))]\n",
    "bindict = {k : 0  for k in range(bins)}\n",
    "for i in freq:\n",
    "    bin_num = i[1] // int((max_val-min_val)/bins)\n",
    "    try:\n",
    "        bindict[bin_num] += 1\n",
    "    except KeyError:\n",
    "        bindict[bin_num-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 596, 1192, 1788, 2384, 2980, 3576, 4172, 4768, 5364, 5960, 6556, 7152, 7748, 8344, 8940, 9536, 10132, 10728, 11324, 11920]\n{0: 33048, 1: 94, 2: 32, 3: 20, 4: 11, 5: 5, 6: 6, 7: 1, 8: 0, 9: 2, 10: 0, 11: 1, 12: 0, 13: 2, 14: 0, 15: 0, 16: 0, 17: 1, 18: 0, 19: 1}\n[('අඩු', 543), ('එම්', 543), ('ආදායම', 548), ('සහිත', 551), ('එස්', 553), ('නීති', 558), ('එහි', 559), ('ගන්නා', 560), ('පෙර', 561), ('ක්\\u200dරීඩා', 563), ('සම්බන්ධ', 566), ('මාරු', 566), ('කළමනාකරණය', 567), ('කර්මාන්ත', 567), ('වැඩ', 567), ('සමාගම', 569), ('ව්\\u200dයාපෘතියේ', 576), ('විවිධ', 578), ('නම්', 579), ('අයවැය', 582), ('වැටුප්', 584), ('වෙන්', 586), ('ආර්ථික', 594), ('වාර්ෂික', 595), ('මුළු', 596), ('පාලන', 601), ('පිළියෙල', 603), ('යොමු', 605), ('කාර්යාල', 608), ('ගත', 609), ('කොට', 611), ('කාර්යාලය', 621), ('ඡන්ද', 622), ('සහකාර', 622), ('බදු', 624), ('අනෙකුත්', 627), ('ඉහළ', 627), ('ආරම්භ', 633), ('මාර්ග', 636), ('ගත්', 641), ('මිල', 643), ('වෙළඳ', 646), ('ආ', 650), ('කරමින්', 655), ('සංඛ්\\u200dයාව', 670), ('මණ්ඩල', 674), ('ලැබේ', 675), ('සභාව', 694), ('ඉටු', 694), ('පරිපාලන', 698), ('සම්පත්', 700), ('රජයේ', 708), ('අමාත්\\u200dයාංශයේ', 720), ('පවත්වා', 727), ('විශේෂ', 734), ('වත්කම්', 734), ('වසරේ', 735), ('වෙනුවෙන්', 736), ('අ', 738), ('විගණන', 754), ('ජල', 754), ('ප්\\u200dරකාශන', 768), ('නිකුත්', 780), ('දෙපාර්තමේන්තුවේ', 781), ('වැය', 782), ('විට', 788), ('වෙනත්', 788), ('සමාලෝචිත', 794), ('වසර', 803), ('සමාජ', 808), ('අධ්\\u200dයක්ෂ', 813), ('සැලසුම්', 813), ('අය', 814), ('සකස්', 843), ('මැතිවරණ', 847), ('කිරීම්', 854), ('ප්\\u200dරතිපාදන', 859), ('අංශය', 861), ('ඇතුළත්', 865), ('ආයතනය', 868), ('පහසුකම්', 870), ('වීම', 879), ('සංවර්ධනය', 883), ('මහතා', 885), ('සමඟ', 885), ('නොතිබුණි', 886), ('වාර්තා', 890), ('මණ්ඩලය', 899), ('පත්', 905), ('වැඩි', 910), ('මේ', 915), ('දරන', 921), ('දිස්ත්\\u200dරික්', 931), ('ලබන', 945), ('ආදායම්', 955), ('දිනට', 962), ('ගැනීමට', 963), ('සම්බන්ධයෙන්', 966), ('පළාත්', 971), ('නව', 972), ('තොරතුරු', 1004), ('එක්', 1050), ('මත', 1054), ('සියලු', 1064), ('කළමනාකරණ', 1072), ('බව', 1097), ('දෙසැම්බර්', 1107), ('ව්\\u200dයාපෘති', 1113), ('අමාත්\\u200dයාංශය', 1113), ('වශයෙන්', 1117), ('මගින්', 1134), ('කාර්ය', 1138), ('ගිණුම්', 1159), ('අවසන්', 1159), ('දීම', 1162), ('ආයතන', 1171), ('සඳහන්', 1177), ('දෙපාර්තමේන්තුව', 1189), ('පරිදි', 1197), ('වියදම්', 1198), ('ප්\\u200dරධාන', 1199), ('වැඩසටහන', 1202), ('ප්\\u200dරාදේශීය', 1219), ('නිලධාරීන්', 1245), ('හෝ', 1249), ('හැකි', 1258), ('පහත', 1273), ('යන', 1300), ('ව්\\u200dයාපෘතිය', 1318), ('තිබුණි', 1336), ('පුහුණු', 1349), ('වී', 1350), ('අවශ්\\u200dය', 1366), ('ණය', 1377), ('ලේකම්', 1381), ('භාෂා', 1424), ('ඉදිරිපත්', 1431), ('මඟින්', 1460), ('අංක', 1471), ('මිලියන', 1509), ('වෙත', 1517), ('ගැනීම', 1545), ('එම', 1562), ('වැඩසටහන්', 1593), ('සිට', 1599), ('කරනු', 1614), ('ගෙන', 1622), ('රුපියල්', 1643), ('සේවා', 1758), ('මුදල්', 1770), ('විය', 1814), ('දක්වා', 1816), ('ය', 1864), ('වර්ෂය', 1888), ('ලදි', 1904), ('වර්ෂයේ', 2053), ('ලංකා', 2071), ('දින', 2137), ('ජාතික', 2142), ('ක', 2145), ('වේ', 2160), ('ව', 2182), ('විසින්', 2192), ('ක්\\u200dරියාත්මක', 2201), ('ම', 2209), ('ලෙස', 2228), ('කිරීමට', 2275), ('තුළ', 2311), ('රු', 2317), ('කිරීමේ', 2342), ('යටතේ', 2387), ('සංවර්ධන', 2395), ('ඒ', 2420), ('අනුව', 2456), ('අදාළ', 2532), ('මූල්\\u200dය', 2550), ('ශ්\\u200dරී', 2700), ('රාජ්\\u200dය', 2722), ('යුතු', 2723), ('සිදු', 2728), ('පිළිබඳ', 2874), ('ලද', 2985), ('කළ', 3110), ('අතර', 3187), ('වූ', 3262), ('ඇති', 3389), ('මෙම', 3672), ('කටයුතු', 3939), ('ද', 3971), ('ලබා', 4104), ('ඇත', 4109), ('කරන', 4152), ('වන', 4550), ('ක්', 5426), ('දී', 5819), ('කර', 6785), ('කිරීම', 8136), ('සහ', 8233), ('සඳහා', 10366), ('හා', 11929)]\n"
     ]
    }
   ],
   "source": [
    "print(binsep)\n",
    "print(bindict)\n",
    "print(freq[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = read_sentences(\"dataset/train/26_MAY/corpus_100_bad.txt\")\n",
    "b = read_sentences(\"dataset/train/26_MAY/corpus_100_good.txt\")\n",
    "new_x = []\n",
    "new_y = []\n",
    "for i, (y,x) in enumerate(zip(a,b)):\n",
    "    if len(y.split()) != len(x.split()):\n",
    "        print(i)\n",
    "        print(y.split())\n",
    "        print(x.split())\n",
    "        break\n",
    "    else:\n",
    "        # for x_i, y_i in zip(x.split(),y.split()):\n",
    "            # print(x_i, '=>', y_i)\n",
    "        new_x.append(x)\n",
    "        new_y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29137"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "len(get_words(\"train_corpus_y_00.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('corpus_100.json', mode = 'r', encoding = 'utf8') as f:\n",
    "    d = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "41580"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===== CREATING VOCAB =====\n"
     ]
    }
   ],
   "source": [
    "PATH_TEST = 'dataset/test_00_good'\n",
    "print(\"===== CREATING VOCAB =====\")\n",
    "vocab = {}\n",
    "id2vocab = {}\n",
    "words_test = []\n",
    "for file in os.listdir(PATH_TEST):\n",
    "    words_test.extend(get_words(os.path.join(PATH_TEST, file)))\n",
    "vocab = {k : i for i, k in enumerate(set(words_test))}\n",
    "if \"<eos>\" not in vocab.keys():\n",
    "    vocab[\"<eos>\"] = len(vocab.keys()) - 1\n",
    "if \"<unk>\" not in vocab.keys():\n",
    "    vocab[\"<unk>\"] = len(vocab.keys()) - 1\n",
    "id2vocab = {value : key for key, value in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_words('dataset/train/text_booksout_good.txt')\n",
    "e = get_words('dataset/train/24_MAY/corpus_10_good.txt')\n",
    "c = get_words('dataset/Other/data_test_good.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = set(list(e)).intersection(set(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4811"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8572"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "len(set(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}